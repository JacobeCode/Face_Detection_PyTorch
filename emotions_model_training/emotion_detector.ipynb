{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import time\n",
    "\n",
    "# For first run only to set proper directory\n",
    "os.chdir('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from emotions_model_training.ERNN import ERNN\n",
    "from emotions_model_training.dl_ops import dl_ops\n",
    "from SQL.connector import connector\n",
    "from emotions_model_training.EmotionsDataset import EmotionsDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramteres for scripts (implement proportion of sets)\n",
    "sets_proportion = [0.6, 0.2, 0.2]       # train / valid / test\n",
    "batch = 64                              # batch size\n",
    "do_shuffle = True                       # shuffling data in loader\n",
    "learn_rate = 1e-3                       # learning rate for training\n",
    "num_epoch = 1                           # training epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repos\\Face_Detection_PyTorch\\SQL\\connector.py:20: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  data = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>face_id_image</th>\n",
       "      <th>box_top</th>\n",
       "      <th>box_left</th>\n",
       "      <th>box_right</th>\n",
       "      <th>box_bottom</th>\n",
       "      <th>box_confidence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>distaste_mother_319.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>120</td>\n",
       "      <td>664</td>\n",
       "      <td>712</td>\n",
       "      <td>168</td>\n",
       "      <td>33.1615</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>distaste_mother_498.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>175</td>\n",
       "      <td>103</td>\n",
       "      <td>150</td>\n",
       "      <td>222</td>\n",
       "      <td>7.75454</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>distaste_mother_58.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>88</td>\n",
       "      <td>136</td>\n",
       "      <td>120</td>\n",
       "      <td>72.9418</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>distaste_mother_623.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>254</td>\n",
       "      <td>5</td>\n",
       "      <td>72</td>\n",
       "      <td>321</td>\n",
       "      <td>79.3727</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distaste_mother_667.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>180</td>\n",
       "      <td>315</td>\n",
       "      <td>202</td>\n",
       "      <td>61.9211</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91788</th>\n",
       "      <td>surprised_expression_546.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>70</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>37.7117</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91789</th>\n",
       "      <td>surprised_expression_381.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>117</td>\n",
       "      <td>107</td>\n",
       "      <td>91.6307</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91790</th>\n",
       "      <td>surprised_expression_395.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>95</td>\n",
       "      <td>258</td>\n",
       "      <td>190</td>\n",
       "      <td>96.2861</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91791</th>\n",
       "      <td>ecstatic_asian_31.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>136</td>\n",
       "      <td>184</td>\n",
       "      <td>108</td>\n",
       "      <td>39.9223</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91792</th>\n",
       "      <td>surprised_expression_394.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>38</td>\n",
       "      <td>152</td>\n",
       "      <td>161</td>\n",
       "      <td>77.7758</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>91793 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              image face_id_image box_top box_left box_right  \\\n",
       "0           distaste_mother_319.jpg             3     120      664       712   \n",
       "1           distaste_mother_498.jpg             1     175      103       150   \n",
       "2            distaste_mother_58.jpg             0      72       88       136   \n",
       "3           distaste_mother_623.jpg             0     254        5        72   \n",
       "4           distaste_mother_667.jpg             0      67      180       315   \n",
       "...                             ...           ...     ...      ...       ...   \n",
       "91788  surprised_expression_546.jpg             0      70       70       351   \n",
       "91789  surprised_expression_381.jpg             0      51       61       117   \n",
       "91790  surprised_expression_395.jpg             0      27       95       258   \n",
       "91791         ecstatic_asian_31.jpg             0      60      136       184   \n",
       "91792  surprised_expression_394.jpg             0      47       38       152   \n",
       "\n",
       "      box_bottom box_confidence label  \n",
       "0            168        33.1615     6  \n",
       "1            222        7.75454     3  \n",
       "2            120        72.9418     6  \n",
       "3            321        79.3727     6  \n",
       "4            202        61.9211     1  \n",
       "...          ...            ...   ...  \n",
       "91788        351        37.7117     5  \n",
       "91789        107        91.6307     5  \n",
       "91790        190        96.2861     5  \n",
       "91791        108        39.9223     3  \n",
       "91792        161        77.7758     5  \n",
       "\n",
       "[91793 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading the dataset with SQL query and dispaying for insight\n",
    "db_dir = \"D:\\\\Repos\\\\Face_Detection_PyTorch\\\\dataset\\\\expression_dataset\\\\data\\\\data\\\\image\\\\origin\\\\\"\n",
    "\n",
    "conn = connector()\n",
    "db = conn.query(f'SELECT * FROM dbo.label')\n",
    "\n",
    "display(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking sizes of prepared datasets:\n",
      "Train shape: 55075 (60%)\n",
      "Valid shape: 18359 (20%)\n",
      "Test shape: 18359 (20%)\n"
     ]
    }
   ],
   "source": [
    "# Splitting data into sets (train, test, valid) - with renumeration\n",
    "train, val_test = train_test_split(db, test_size=0.4, train_size=0.6, random_state=42, stratify=db['label'])\n",
    "valid, test = train_test_split(val_test, test_size=0.5, train_size=0.5, random_state=42, stratify=val_test['label'])\n",
    "\n",
    "train, valid, test = train.reset_index(drop=True), valid.reset_index(drop=True), test.reset_index(drop=True)\n",
    "\n",
    "print(\"Checking sizes of prepared datasets:\")\n",
    "print(\"Train shape: \" + str(train.shape[0]) + \" (\" + str(round((train.shape[0]/db.shape[0])*100)) + \"%)\")\n",
    "print(\"Valid shape: \" + str(valid.shape[0]) + \" (\" + str(round((valid.shape[0]/db.shape[0])*100)) + \"%)\")\n",
    "print(\"Test shape: \" + str(test.shape[0]) + \" (\" + str(round((test.shape[0]/db.shape[0])*100)) + \"%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init for datasets with PyTorch class\n",
    "train_dataset = EmotionsDataset(train, db_dir)\n",
    "\n",
    "# Fixing valid and test dataset to same sizes (for convinience with NN architecture)\n",
    "horz, vert = train_dataset.horz_max, train_dataset.vert_max\n",
    "\n",
    "valid_dataset = EmotionsDataset(valid, db_dir, horz, vert)\n",
    "test_dataset = EmotionsDataset(test, db_dir, horz, vert)\n",
    "\n",
    "# Init for dataloaders with default batch = 64\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=batch, shuffle=do_shuffle, drop_last=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=batch, shuffle=do_shuffle, drop_last=True)\n",
    "test_dataloader = DataLoader(dataset=test_dataset, batch_size=batch, shuffle=do_shuffle, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of images: 2117 x 2117\n"
     ]
    }
   ],
   "source": [
    "# Checking maximal size - in theory it can vary, but it depends from split of dataset\n",
    "print(\"Size of images: \" + str(horz) + \" x \" + str(vert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "# Checking CUDA avalibility\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(\"Using \" + str(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture of current ERNN: \n",
      " ERNN(\n",
      "  (conv1): Conv2d(3, 1, kernel_size=(197, 197), stride=(120, 120))\n",
      "  (relu1): ReLU()\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (lin1): Linear(in_features=289, out_features=7, bias=True)\n",
      "  (soft): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Defining model and parameters\n",
    "ernn = ERNN().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adagrad(ernn.parameters(), lr=learn_rate)\n",
    "\n",
    "# Creating operational class for proceding with train, valid, test process\n",
    "dl_ops = dl_ops(model=ernn, loss_fn=loss_fn, optimizer=optim)\n",
    "print(f\"Architecture of current ERNN: \\n {ernn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repos\\Face_Detection_PyTorch\\face_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdl_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos\\Face_Detection_PyTorch\\emotions_model_training\\dl_ops.py:10\u001b[0m, in \u001b[0;36mdl_ops.train\u001b[1;34m(self, dataloader, epoch, device)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataloader, epoch, device):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Setting proper mode\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miteration\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos\\Face_Detection_PyTorch\\face_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\Repos\\Face_Detection_PyTorch\\face_venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Repos\\Face_Detection_PyTorch\\face_venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\Repos\\Face_Detection_PyTorch\\emotions_model_training\\EmotionsDataset.py:37\u001b[0m, in \u001b[0;36mEmotionsDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;66;03m# Getting proper image from database and box for faces\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m     pict \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopt_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdb\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     box \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_left\u001b[39m\u001b[38;5;124m'\u001b[39m][index]), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_top\u001b[39m\u001b[38;5;124m'\u001b[39m][index]), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_right\u001b[39m\u001b[38;5;124m'\u001b[39m][index]), \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbox_bottom\u001b[39m\u001b[38;5;124m'\u001b[39m][index]))\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# Preparing \"only face\" image by cropping and getting labels\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos\\Face_Detection_PyTorch\\face_venv\\Lib\\site-packages\\PIL\\Image.py:3277\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3274\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mrealpath(os\u001b[38;5;241m.\u001b[39mfspath(fp))\n\u001b[0;32m   3276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3277\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3278\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3280\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(dl_ops.train(train_dataloader, epoch=num_epoch, device=device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
